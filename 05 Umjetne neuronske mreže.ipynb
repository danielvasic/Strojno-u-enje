{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Što je Perceptron?\n",
    "\n",
    "Perceptron je jedan od najranijih oblika neuronskih mreža, koji je izvorno razvijen kao model za strojno učenje i predstavlja osnovu za moderne duboke neuronske mreže. To je vrsta umjetnog neurona koji koristi jednostavnu linearnu funkciju za izračunavanje ponderiranog zbroja ulaznih signala, kojima se zatim dodaje pristranost (bias). Rezultat te sume zatim prolazi kroz neku vrstu nelinearne aktivacijske funkcije, često sigmoidnu funkciju, kako bi se generirao izlaz.\n",
    "\n",
    "#### Osnovne značajke Perceptrona:\n",
    "\n",
    "1. **Ulazi i težine**: Perceptron prima višestruke ulazne signale, svaki od kojih ima pripadajuću težinu koja indicira važnost svakog ulaza. Ti ulazi se množe sa svojim težinama i zbroje se da bi se formirao ukupan ponderirani ulaz.\n",
    "\n",
    "2. **Pristranost (Bias)**: Pored ponderiranih ulaza, perceptron uključuje i bias, koji se dodaje na zbroj kako bi se omogućilo fino podešavanje izlaza. Bias omogućava perceptronu da bolje prilagodi podatke koji nisu idealno linearno separabilni.\n",
    "\n",
    "3. **Aktivacijska funkcija**: Nakon što se izračuna ponderirani zbroj ulaza i biasa, rezultat se šalje kroz aktivacijsku funkciju. U mnogim slučajevima, to je sigmoidna funkcija koja izlaz ograničava na raspon između 0 i 1, čineći ga pogodnim za probleme binarne klasifikacije.\n",
    "\n",
    "4. **Učenje i prilagođavanje težina**: Perceptron se \"uči\" kroz proces prilagođavanja težina i biasa temeljen na greškama između predviđenih i stvarnih izlaza. Taj proces, poznat kao algoritam povratne propagacije, omogućava perceptronu da iterativno poboljšava svoje predviđanja prilagođavajući težine u skladu s greškama.\n",
    "\n",
    "#### Primjena:\n",
    "\n",
    "Perceptroni su jednostavni i efikasni za probleme binarne klasifikacije, kao što su prepoznavanje oblika i predikcija kategorije na osnovu skupa ulaznih značajki. Ipak, oni imaju ograničenja u radu s problemima koji nisu linearno separabilni, što je dovelo do razvoja višeslojnih perceptrona (MLP) i dubokih neuronskih mreža koji koriste više slojeva perceptrona za rješavanje složenijih problema.\n",
    "\n",
    "Perceptron ostaje temeljni blok u učenju dubokih neuronskih mreža i povijesno značajan korak u razvoju algoritama strojnog učenja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5031444872132191\n",
      "0.5\n",
      "0.4968555127867808\n",
      "0.49371127430020534\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = np.random.rand(input_size + 1) - 0.5\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        return sigmoid(summation)\n",
    "    \n",
    "    def train(self, inputs, targets, epochs):\n",
    "        for epoch in range (epochs):\n",
    "            for input, target in zip(inputs, targets):\n",
    "                prediction = self.predict(input)\n",
    "                error = target-prediction\n",
    "                self.weights[1:] += self.learning_rate * error * sigmoid_derivative(prediction) * input\n",
    "                self.weights[0] += self.learning_rate * error * sigmoid_derivative(prediction)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    targets = np.array([0, 1, 1, 0])\n",
    "\n",
    "    perceptron = Perceptron(input_size=2)\n",
    "    perceptron.train(inputs, targets, epochs=10000)\n",
    "\n",
    "    for input in inputs:\n",
    "        print(perceptron.predict(input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Što je Višeslojni Perceptron (MLP)?\n",
    "\n",
    "Višeslojni perceptron (MLP) je vrsta umjetne neuronske mreže koja se sastoji od više slojeva neurona, uključujući ulazni sloj, jedan ili više skrivenih slojeva i izlazni sloj. Za razliku od jednoslojnog perceptrona koji može modelirati samo linearno separabilne funkcije, MLP može naučiti i predstaviti složene nelinearne modele zahvaljujući svojim skrivenim slojevima.\n",
    "\n",
    "#### Osnovne značajke MLP-a:\n",
    "\n",
    "1. **Slojevi**: MLP ima tri osnovna tipa slojeva:\n",
    "   - **Ulazni sloj**: prima ulazne podatke.\n",
    "   - **Skriveni slojevi**: Ovi slojevi izvode većinu teškog računanja putem neurona koji nisu direktno izloženi ulazima ili izlazima. Broj skrivenih slojeva i broj neurona u svakom sloju može varirati, ovisno o složenosti problema.\n",
    "   - **Izlazni sloj**: proizvodi konačni rezultat mreže.\n",
    "\n",
    "2. **Težine i biasi**: Svaki neuron u jednom sloju povezan je s neuronima u sljedećem sloju preko težina, koje se prilagođavaju tijekom procesa učenja. Biasi su dodani za prilagodbu izlaza uz težine.\n",
    "\n",
    "3. **Aktivacijska funkcija**: Koristi se za dodavanje nelinearnosti u mrežu, omogućujući modeliranje kompleksnijih funkcija. Sigmoidna funkcija je česta izbira za binarne klasifikacijske zadatke, iako se u modernim aplikacijama često koriste ReLU funkcije i njihove varijante.\n",
    "\n",
    "#### Proces učenja u MLP-u:\n",
    "\n",
    "1. **Naprijedna propagacija**: Počevši od ulaznog sloja, signali se šalju kroz mrežu do izlaznog sloja. Na svakom neurona izračunava se ponderirani zbroj ulaza i biasa, koji se zatim šalje kroz aktivacijsku funkciju.\n",
    "\n",
    "2. **Povratna propagacija (backpropagation)**: Nakon dobivanja izlaza, izračunava se greška u odnosu na stvarne ciljeve. Greška se šalje natrag kroz mrežu, ažurirajući težine i biasi kako bi se minimizirala greška izlaza u sljedećim prolazima.\n",
    "\n",
    "3. **Ažuriranje težina**: Težine se ažuriraju kako bi se smanjila ukupna greška, obično koristeći algoritme kao što je gradijentni spust ili njegove varijante.\n",
    "\n",
    "#### Primjena:\n",
    "\n",
    "MLP se široko koristi u dubokom učenju i može se primijeniti na različite zadatke kao što su klasifikacija slika, obrada prirodnog jezika, i predviđanje vremenskih serija. Sposobnost učenja složenih obrazaca i otkrivanja suptilnih veza u podacima čini MLP vrlo moćnim alatom u strojnom učenju.\n",
    "\n",
    "MLP predstavlja temelj za mnoge složene arhitekture dubokog učenja, uključujući konvolucijske neuronske mreže (CNN) i rekurentne neuronske mreže (RNN), koje su prilagođene specifičnim vrstama podataka i zadacima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0] Output: [0.05960368]\n",
      "Input: [0 1] Output: [0.94346653]\n",
      "Input: [1 0] Output: [0.9201462]\n",
      "Input: [1 1] Output: [0.08467818]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, output_size, lr=0.1):\n",
    "        self.lr = lr\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.rand(layer_sizes[i], layer_sizes[i + 1]) - 0.5)\n",
    "            self.biases.append(np.zeros(layer_sizes[i + 1]))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        current_layer = inputs\n",
    "        for weights, bias in zip(self.weights, self.biases):\n",
    "            current_layer = self.sigmoid(np.dot(current_layer, weights) + bias)\n",
    "        return current_layer\n",
    "\n",
    "    def train(self, training_inputs, labels, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                activations = [inputs]\n",
    "                current_layer = inputs\n",
    "\n",
    "                # Forward propagation\n",
    "                for weights, bias in zip(self.weights, self.biases):\n",
    "                    current_layer = self.sigmoid(np.dot(current_layer, weights) + bias)\n",
    "                    activations.append(current_layer)\n",
    "\n",
    "                # Backward propagation\n",
    "                errors = label - activations[-1]\n",
    "                for i in reversed(range(len(self.weights))):\n",
    "                    current_error = errors * self.sigmoid_derivative(activations[i + 1])\n",
    "                    if i != 0:\n",
    "                        errors = np.dot(current_error, self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "                    delta = np.dot(activations[i].T.reshape(-1, 1), current_error.reshape(1, -1))\n",
    "                    self.weights[i] += self.lr * delta\n",
    "                    self.biases[i] += self.lr * current_error\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nn = NeuralNetwork(input_size=2, hidden_layers=[3], output_size=1)\n",
    "    training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    labels = np.array([[0], [1], [1], [0]])  # XOR problem\n",
    "\n",
    "    nn.train(training_inputs, labels, epochs=20000)\n",
    "    for input in training_inputs:\n",
    "        print(f'Input: {input} Output: {nn.predict(input)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
